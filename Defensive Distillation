import tensorflow as tf
from tensorflow.keras import datasets, models, layers
from tensorflow.keras.applications import ResNet50V2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.datasets import mnist
from tensorflow.keras import models, layers

# MNIST Dataset
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0

# Convert images to RGB
train_images = tf.image.grayscale_to_rgb(tf.image.resize(train_images[..., tf.newaxis], (32, 32)))
test_images = tf.image.grayscale_to_rgb(tf.image.resize(test_images[..., tf.newaxis], (32, 32)))

# Convert labels to one-hot encoded format
train_labels = to_categorical(train_labels, num_classes=10)
test_labels = to_categorical(test_labels, num_classes=10)


def build_resnet(teacher=False):
    model_input = layers.Input(shape=(32, 32, 3))
    
    x = layers.Conv2D(32, (3, 3), activation='relu')(model_input)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)
    
    x = layers.Conv2D(64, (3, 3), activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)
    
    x = layers.GlobalAveragePooling2D()(x)
    
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.25)(x)
    
    predictions = layers.Dense(10, activation='softmax')(x)
    
    model = models.Model(inputs=model_input, outputs=predictions)
    return model


# Define FGSM attack
def fgsm_attack(model, x, y_true, epsilon, temperature):
    loss_object = tf.keras.losses.CategoricalCrossentropy()
    input_data = tf.convert_to_tensor(x)

    with tf.GradientTape() as tape:
        tape.watch(input_data)
        prediction = model(input_data)
        teacher_probs = tf.nn.softmax(prediction / temperature)
        loss = loss_object(y_true, teacher_probs)

    gradient = tape.gradient(loss, input_data)
    x_adv = input_data + epsilon * tf.sign(gradient)
    x_adv = tf.clip_by_value(x_adv, 0, 1)
    return x_adv

# Create teacher and student models
teacher_model = build_resnet(teacher=True)
student_model = build_resnet(teacher=False)

# Compile the Defensive Distillation model
student_model.compile(optimizer='adam', metrics=['accuracy'])

def defensive_distillation(teacher_model, student_model, train_images, train_labels, teacher_temperature, student_temperature, epochs, batch_size):
    optimizer = tf.keras.optimizers.Adam()
    loss_object = tf.keras.losses.CategoricalCrossentropy()

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        for batch_start in range(0, len(train_images), batch_size):
            batch_images = train_images[batch_start:batch_start + batch_size]
            batch_labels = train_labels[batch_start:batch_start + batch_size]

            with tf.GradientTape() as tape:
                teacher_logits = teacher_model(batch_images)
                teacher_probs = tf.nn.softmax(teacher_logits / teacher_temperature)
                student_logits = student_model(batch_images)
                loss = loss_object(teacher_probs, tf.nn.softmax(student_logits / student_temperature))

            gradients = tape.gradient(loss, student_model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))

           # Evaluate accuracy after each batch
            metrics_values = student_model.evaluate(batch_images, batch_labels, verbose=0)
            metrics_names = student_model.metrics_names
            metrics_dict = dict(zip(metrics_names, metrics_values))

            print(f"Batch {batch_start // batch_size + 1}/{len(train_images) // batch_size}, Training Metrics: {metrics_dict}")

    return student_model

# Example usage:
teacher_temperature = 10
student_temperature = 5
epochs = 10
batch_size = 256

# Training hyperparameters
epsilon_fgsm = 0.3

# Defensive Distillation
#Defensive_Distillation_Model = defensive_distillation(teacher_model, student_model, train_images, train_labels, temperature, epochs, batch_size)

# Compile
#Defensive_Distillation_Model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


# Evaluate the model with FGSM attack on the test data
#test_images_adv_defense = fgsm_attack(Defensive_Distillation_Model, test_images, test_labels, epsilon_fgsm, temperature)

# Evaluate the model on the adversarial examples
#test_loss_adv_defense, test_accuracy_adv_defense = Defensive_Distillation_Model.evaluate(test_images_adv_defense, test_labels)
#print(f"Test accuracy of a Defensive Distillation Trained Model with FGSM attack: {test_accuracy_adv_defense}")

# Defensive Distillation
Defensive_Distillation_Model = defensive_distillation(teacher_model, student_model, train_images, train_labels, teacher_temperature, student_temperature, epochs, batch_size)

# Evaluate the model on the test data
test_loss_defense, test_accuracy_defense = Defensive_Distillation_Model.evaluate(test_images, test_labels)
print(f"Test accuracy of a Defensive Distillation Trained Model: {test_accuracy_defense}")
